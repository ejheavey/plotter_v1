///* This program is set up to capture real-time images containing a desired object. The object is detected, and the image is filtered to only show the particular
//object we want to see (i.e. a ball, or a pen). Using the filtered image, a single point is tracked, and the position of the point is stored in an array. Using
//the collected data, the x- and y-coordinates are plotted in new window and the program returns a prediction of a shape*/
//
#include <opencv\cv.h>
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include<opencv2/imgproc/imgproc.hpp>
#include<opencv2/objdetect.hpp>
#include<iostream>
#include <math.h>
#include <sstream>
#include <stdint.h>
#include <string>

using namespace cv;
using namespace std;
// END INDCLUDE HEADERS
/////////////////////////////////////////
//
//

// DEFINE OBJECT CLASS WITH ATTRIBUTES AND PARAMETERS WE WISH TO TRACK
// SUCH AS THE CENTER POINT COORDINATES AND THE BOUNDING RECT SIZE
//////////////////////////////////////////////////////////////////////////
class Object {

public:
	// CLASS ATTRIBUTES
	vector<Point> contour; //	 OBJECT CONTOUR (outer edge)
	Rect objBoundingRect;  //	 OBJECT BOUNDING RECTANGLE (box)
	vector<Point> centerpts; //  centerpts = center.x, center.y
	Point center; //			 
	Point predictedNewPos; // 
	double dblDiagSize; //		 LENGTH OF BOUNDING RECT DIAGS
	double dblAspectRatio; //	 W:H RATIO OF OBJ BOUNDING RECT

	////////////////////
	// METHOD PROTOTYPES
	// OBJECT CLASS INSTANTIATION
	~Object(void);
	Object(string name);
	Object(vector<Point> _contour);
	
	// LOCATION/POSITION METHODS
	void predictNextPosition();
	void setXpos(int x);
	void setYpos(int y);
	int getXpos();
	int getYpos();

	// 
	string getType() { return type; }
	void setType(string t) { type = t; }

	//
	void setColor(Scalar c) { Color = c; }
	Scalar getColor() { return Color; }


private:
	int Xpos, Ypos;
	string type;
	Scalar Color;
};// END CLASS DESCRIPTOR
////////////////////////////////////////
//
//

// CLASS METHOD DEFINITIONS
///////////////////////////////////
Object::Object(vector<Point> _contour) {
	contour = _contour; //	STORE OBJECT CONTOURS
	objBoundingRect = cv::boundingRect(contour); //	 ESTABLISH BOUNDING RECT FOR KNOWN OBJECT CONTOURS
	center.x = (objBoundingRect.x + objBoundingRect.x + objBoundingRect.width) / 2;
	center.y = (objBoundingRect.y + objBoundingRect.y + objBoundingRect.height) / 2;
	centerpts.push_back(center); // PUSHES center.x AND center.y TO VECTOR centerpts
	dblDiagSize = sqrt(pow(objBoundingRect.width, 2) + pow(objBoundingRect.height, 2)); // c = sqrt(x^2 + y^2)
	dblAspectRatio = (float)objBoundingRect.width / (float)objBoundingRect.height; // W:H RATIO OF BOUNDING RECT
}
//
Object::Object(string name) {
	setType(name);
}
//
Object::~Object(void) {

}
//
void Object::setXpos(int x) { Object::Xpos = x; }
void Object::setYpos(int y) { Object::Ypos = y; }
//
int Object::getXpos() { return Object::Xpos; }
int Object::getYpos() { return Object::Ypos; }
//
void Object::predictNextPosition() {
	int numPosns = (int)centerpts.size();
	
	// CONSIDERING ONLY CURRENT POSITION
	if (numPosns == 1) { 
		predictedNewPos.x = centerpts.back().x;
		predictedNewPos.y = centerpts.back().y;
	}
	// CONSIDERING CURRENT AND ONE PREVIOUS
	else if (numPosns == 2) { 
		int dX = centerpts[1].x - centerpts[0].x;
		int dY = centerpts[1].y - centerpts[0].y;
		predictedNewPos.x = centerpts.back().x + dX;
		predictedNewPos.y = centerpts.back().y + dY;
	}
}
// END CLASS METHODS
//////////////////////////////////
//
//

////////////////////////////////////////
// GLOBAL VARIABLES
VideoCapture cam; //	 Camera object of class VideoCapture
Mat frame1, frame2; //	 initial frames (UNFILTERED)
Mat gray1, gray2, thresh1, thresh2; //	 grayscale and thresh imgs
Mat diffImg, threshDiff; //		difference img and threshold
vector<Object> detObjs; //	 Vector containing detected objects
vector<vector<Point>> contours; //	Initial contour storage
vector<Vec4i> hierarchy; // 

int frameWidth, frameHeight, frameDepth; //		IMG DIMENSIONS
int SensitivityVal = 15; //		MINIMUM THRESHOLD VALUE
bool trackingEnabled = true, debugMode = true, pause = false;

bool objectFound;
int min_obj_area = 60 * 60;

////////////////////////////////////////////////
// SUBROUTINE PROTOTYPES
void initial(void); // subRoutine_0(); Check if the camera is open/on
int framesAvailable(void); // subRoutine_1(); Make sure there is a frame available for analysis
void filterFrames(Mat img1, Mat img2); // subRoutine_2(); Filter the frame accordingly
void detectObj();
void computation();
string numToString(float num);

////////////////////////////////////////////////
//*Begin main function */
int main(int argc, const char** argv)
{
	/* subRoutine_0() */ initial();

	// Run this continuously
	while (1) { // Begin endless loop...

		/* subRoutine_1() */ framesAvailable();
		/* This subroutine is simply in place to determine whether or not the camera
		is taking images. If a frame is available, subRoutine_2() gets called to filter the image.*/

		/* subRoutine_2() */ filterFrames(frame1, frame2);
		/* This subroutine first reduces the amount of data to handle by first converting
			frame1 and frame2 to single-channel grayscale images. The images are then
			smoothed with a Gaussian LPF, and the absdiff of the images is computed and
			thresholded such that distinct contours can be seen.*/
		/* 
			Depending on the value of boolean flags trackingEnabled, debugMode, and pause,
		    the program will operate in different modes and output different video frames. 
		*/
		switch (waitKey(10)) {
		case 27: // PRESS ESC
			return 0;
		case 116: // PRESS t
			trackingEnabled = !trackingEnabled;
			if (trackingEnabled == false) {
				cout << "Tracking disabled.\n" << endl;
				destroyAllWindows();
			}
			else {
				cout << "Tracking enabled.\n" << endl;
				destroyAllWindows();
			}
			break;
		case 100: // PRESS d
			debugMode = !debugMode;
			if (debugMode == false) {
				cout << "Debug mode disabled.\n" << endl;
				destroyAllWindows();
			}
			else {
				cout << "Debug mode enabled.\n" << endl;
				destroyAllWindows();
			}
			break;
		case 112: // PRESS p
			pause = !pause;
			if (pause == true) {
				while (pause == true) {
					switch (waitKey()) {
					case 112:
						pause = false;
						break;
					}
				}
			}
		} // END SWITCH BLOCK
		if (trackingEnabled == true && debugMode == true) {
			detectObj(); /* subroutine_3() outputs contour image */
			imshow("Unfiltered Tracking", frame2);
			imshow("Debug Threshold Image", threshDiff);
		}
		else if (trackingEnabled == true && debugMode == false) {
			detectObj(); /* subroutine_3() will not show contours */
			imshow("Unfiltered Tracking", frame2);
		}
		else if (trackingEnabled == false && debugMode == true){
			imshow("Tracking and debugging disabled: no detection output", gray2); 
			imshow("Debug Threshold Image", threshDiff);
		}
		else { imshow("Tracking and debugging disabled: no detection output", gray2); }
	}// CLOSE CONTINUOUS WHILE LOOP

	return 0;
}//* END MAIN FUNCTION */
////////////////////////////////////////////
//
//

////////////////////////////////////////////////
/* SUBROUTINE DEFINITIONS */
////////////////////////////////////////////////
//
string numToString(float num) {
	std::stringstream ss;
	ss << num;
	return ss.str();
}//

 /////////////////////////////////////////////////////
// subRoutine_0(): INITIALIZE THE CAMERA, RECORD FPS
void initial(void) { // subRoutine_0()
	cam.open(1);

	if (!cam.isOpened()) { // Check that the webcam is available for streaming video
		cout << "Error opening the webcam" << endl;
		cin.get();
	}
	else { // If the camera is open, output the frame rate
		double fps = cam.get(CAP_PROP_FPS); //get capture object frame rate property value and display it
		cout << "Frames per second: " << fps << endl;
	} cin.get();
}// END subRoutine_0()

 ////////////////////////////////////////////////
// subRoutine_1(): CHECK FRAME ABVAILABILITY
int framesAvailable(void) { // subRoutine_1()
	bool availableFrame = cam.read(frame1);

	if (!availableFrame) {
		cout << "No available frames" << endl;
		cin.get();
		return -1;
	}
	else {
		frameWidth = frame1.cols;
		frameHeight = frame1.rows;
		frameDepth = frame1.channels();
		cam.read(frame2);
	}
	return 0;
}// END subRoutine_1()

////////////////////////////////////////////////
// subRoutine_2(): APPLY FILTERS AND THRESHOLD
void filterFrames(Mat img1, Mat img2) { // subRoutine_2()
	cv::Mat structuringElement5x5 = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(5, 5));
	cv::Mat structuringElement3x3 = cv::getStructuringElement(cv::MORPH_RECT, cv::Size(3, 3));

	cvtColor(img1, gray1, COLOR_BGR2GRAY);
	cvtColor(img2, gray2, COLOR_BGR2GRAY);

	GaussianBlur(gray1, gray1, Size(5, 5), 1, 1);
	GaussianBlur(gray2, gray2, Size(5, 5), 1, 1);

	absdiff(gray1, gray2, diffImg);
	threshold(diffImg, threshDiff, SensitivityVal, 255, THRESH_BINARY);
	//threshold(threshImg, threshImg, SensitivityVal, 255, THRESH_BINARY);

	cv::dilate(threshDiff, threshDiff, structuringElement3x3, Point(-1,-1), 3, 0);
	cv::dilate(threshDiff, threshDiff, structuringElement5x5, Point(-1, 1), 2, 0);
	//cv::dilate(threshImg, threshImg, structuringElement5x5);

}// END subRoutine_2()


////////////////////////////////////////////////
// subRoutine_3(): DETECT OBJECT(S)
void detectObj() {
	Mat cntrImg(frameHeight, frameWidth, CV_8UC1);
	Scalar color = Scalar(0, 0, 0); 
	//findContours(threshDiff, contours, hierarchy, CV_RETR_LIST, CV_CHAIN_APPROX_SIMPLE, Point(0,0));
	findContours(threshDiff, contours, hierarchy, CV_FILLED, CV_CHAIN_APPROX_SIMPLE, Point(0, 0));

	if (hierarchy.size() > 0) {
		for (int i = 0; i >= 0; i = hierarchy[i][0]) {
			Object detObj(contours[i]);
			detObj.setType("Object");
			if (detObj.objBoundingRect.area() > min_obj_area &&
				(contourArea(detObj.contour) / (double)detObj.objBoundingRect.area()) > 0.45) {
				
				drawContours(cntrImg, contours, i, color, 2, 8, hierarchy, 0, Point());
				
				rectangle(frame2, detObj.objBoundingRect, color, 3, 8, 0);
				
				circle(frame2, Point(detObj.center.x +20 , detObj.center.y), 5, color, -1, 8, 0);
				
				putText(frame2, detObj.getType(), Point(detObj.objBoundingRect.x, detObj.objBoundingRect.y),
					CV_FONT_HERSHEY_COMPLEX_SMALL, 1, color, 2, 8);
				
				putText(frame2, numToString(detObj.center.x) + ", " + numToString(detObj.center.y), 
					Point(detObj.center.x, detObj.center.y), CV_FONT_HERSHEY_COMPLEX_SMALL, 1, color, 2, 8);
			}	
		}
	}
		imshow("Detected Contours", cntrImg);

}// END subRoutine_3()








